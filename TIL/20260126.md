# 5장 토큰화 / 6장 임베딩

## 5장 토큰화
### 개요
#### 자연어와 자연어 처리
- 자연어(National Language) : 자연 언어라고 부르며, 사람들에 의해 자연스럽게 만들어진 언어를 의미함
- 자연어 처리(Natural Language Processing, NPL) : 컴퓨터가 인간의 언어를 이해하고 해석 및 생성하기 위한 기술을 의미함

#### 자연어 처리(NPL)의 목적
- 컴퓨터가 인간과 유사한 방식으로 언어를 이해하고 처리하는 것을 목표로 함
- 모델 개발을 위해 해결해야할 문제
  - 모호성(Ambiguity) : 맥락에 따라 다양한 의미를 가질 수 있는 모호성 해결
  - 가변성(Variability) : 사투리, 강세, 신조어 등에 의한 가변성을 처리할 수 있어야함
  - 구조(structure) : 문장의 구조와 문법적 요소를 이해하여 추론, 분석할 수 있어야함
- 말뭉치(Corpus)를 일정한 단위인 토큰(Token)으로 나눠야 함
  - 말뭉치? 자연어 모델을 훈련하고 평가하는 데 사용되는 대규모 자연어를 뜻함

#### 토큰?
- 개별 단어나 문장 부호와 같은 텍스트를 의미하며 말뭉치보다 더 작은 단위
- 텍스트의 개별 단어, 구두점 또는 기타 의미 단위일 수 있음
- 토큰으로 나누는 목적은 컴퓨터가 자연어를 이해할 수 있게 나누는 과정임
- 토큰으로 나누는 과정을 토큰화(Tokenization)라고 함
- 토큰화는 컴퓨터가 텍스트를 보다 효율적으로 분석하고 처리할 수 있도록 하는 중요한 단계

#### 토크나이저(Tokenizer)
- 텍스트 문자열을 토큰으로 나누는 알고리즘 또는 소프트웨어를 의미함
- 일반적으로 토큰을 나누는 여러 기준
  - 공백 분할 : 텍스트를 공백 단위로 분리해 개별 단어로 토큰화
  - 정규표현식 적용 : 정규 표현식으로 특정 패턴을 식별해 텍스트 분할
  - 어휘 사전(Vocabulary) : 사전에 정의된 단어 집합을 토큰으로 사용
  - 머신러닝 활용 : 데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝을 적용

#### OOV(Out of Vocab)
- 어휘 사전에 없는 단어나 토큰이 존재하는 경우

### 단어 및 글자 토큰화
#### 단어 토큰화
- 자연어 처리 분야에서 핵심적인 전처리 작업 중 하나로 텍스트 데이터를 의미 있는 단위인 단어로 분리하는 작업
- 띄어쓰기, 문장 부호, 대소문자 등의 특정 구분자를 활용해 토큰화 수행됨

#### 글자 토큰화
- 띄어쓰기뿐만이 아니라 글자 단위로 문장을 나누는 방식
- 비교적 작은 단어 사전을 구축할 수 있어서 컴퓨팅 자원을 아낄수 있음
- 또한, 전체 말뭉치를 학습할 때 각 단어를 더 자주 학습할 수 있다는 장점이 있음
- 단점은 구조적 의미 파악이 어려움

### 형태소 토큰화
- 텍스트를 형태소 단위로 나누는 토큰화 방식
- 언어의 문법 구조를 고려해 단어를 분리하고 이를 의미 있는 단위로 분류하는 작업
- 특히 한국어와 같이 교착어(Agglutinative Language)인 언어에서 중요하게 수행됨

#### 형태소 종류
- 자립 형태소(Free Morpheme) : 명사, 동사, 형용사와 같이 스스로 의미를 가지고 있음
- 의존 형태소(Bound Morpheme) : 조사, 어미, 접두사, 접미사 등 스스로 의미를 갖지 못하고 다른 형태소와 조합되어 사용됨

#### 형태소 어휘 사전
- 각 단어의 형태소 정보를 포함하는 사전을 의미함
- 일반적으로 각 형태소가 어떤 품사에 속하지는지와 해당 품사의 뜻 등의 정보도 함께 제공됨

#### 품사 태깅(POS Tagging)
- 텍스트 데이터의 형태소를 분석하여 각 형태소에 해당하는 품사(Part of Speech, POS)를 태깅하는 작업을 의미함
- 자연어 처리 분야에서 문맥을 고려할 수 있도록 함

### 하위 단어 토큰화
- 형태소 분석은 띄어쓰기나 맞춤법이 잘 지켜지지 않는 경우, 신조어나 외래어, 전문용어, 축약어 등에 대해 완벽히 대응하기 어려움
- 즉, 형태소 분석기는 모르는 단어를 적절한 단위로 나누는 것에 취약하며, 잠재적으로 어휘 사전의 크기를 크게 만들고 OOV에 대응하기 어렵게 만듦
- 이를 해결하귀 위한 방법으로, **하위 단어 토큰화(Subword Tokenization)가 있음
- 하나의 단어가 빈번하게 사용되는 하위 단어(Subword)의 조합으로 나누어 토큰화 하는 방법
- 예를 들어, 'Reinforcement'라는 단어에 하위 단어 토큰화를 적용하여, 'Rein', 'force', 'ment'등으로 나눠 처리하는 방식임
- 하위 단어 토큰화를 적용하면, 단어의 길이를 줄일 수 있어서 처리 속도가 빨라지고, OOV문제, 신조어, 은어, 고유어 등으로 인한 문제를 완화할 수 있음


  

## 6장 임베딩
### 개요
- 컴퓨터는 텍스트 자체를 이해할 수 없으므로 텍스트를 숫자로 변환하는 텍스트 벡터화(Text Vectorization) 과정이 필요함
- 기초적인 벡터화로는 원-핫 인코딩(One-Hot Encoding), 빈도 벡터화(Count Vectorization) 등이 있음
  - 원-핫 인코딩(One-Hot Encoding) : 문서에 등장하는 각 단어를 고유한 색인 값으로 매핑한 후, 해당 색인 위치를 1로 표시하고 나머지 위치는 모두 0으로 표시하는 방법
  - 빈도 벡터화(Count Vectorization) : 문서에서 단어의 빈도수를 세어 해당 단어의 빈도를 벡터로 표현하는 방식
    - ex) apples라는 단어가 총 4번 등장한 경우 → 해당 단어에 대한 벡터값은 4가 
- 장점 및 단점
  - 장점
    - 단어나 문장을 벡터 형태로 변환하기 쉽고 간단함
  - 단점
    - 벡터의 희소성(Sparsity)이 크다는 단점이 있음
      - 벡터의 **희소성(Sparsity)** : 벡터의 대부분 값이 0이고, 실제로 의미 있는 값은 극히 일부만 존재하는 상태를 말함
      - 비교적 적은 토큰 수의 데이터 입력 시 말뭉치 내에 존재하는 토큰의 개수만큼 벡터 차원이 커지지만, 실제로는 대부분이 0인 희소 벡터가 되기 때문에 계산 비용은 커지고 학습 효율도 떨어짐
    - "벡터 공간에서의 거리(수학적 유사도)"와 "문장의 의미적 유사도"가 일치하지 않는 현상 발생 (Semantic gap)
- 위 단점들을 해결하기 위해 워드 투 벡터(Word2Vec)나 패스트 텍스트(fastText) 등과 같이 단어의 의미를 학습해 표현하는 워드 임베딩(Word Embedding) 기법을 사용함
- 워드 임베딩(Word Embedding) : 단어를 고정된 길이의 실수 벡터로 표현하는 방법으로, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계를 추론함
- 워드 임베딩은 고정된 임베딩을 학습하기 때문에 다의어나 문맥 정보를 다루기 어렵다는 단점이 있음
- 인공 신경망을 활용해 동적 임베딩(Dynamic Embedding) 기법을 사용
- 동적 임베딩(Dynamic Embedding) : 같은 단어라도 문맥(Context)에 따라 서로 다른 벡터로 표현되는 임베딩을 말함
    
### 언어 모델
- 언어 모델이란, 앞에 주어진 문맥을 바탕으로 다음에 올 단어(또는 토큰)의 확률 분포를 학습하는 모델

#### 자기회귀 언어 모델
- 입력된 문자들의 조건부 확률을 이용해 다음에 올 단어를 예측
- 이전까지 생성한 토큰만을 입력으로 사용해, 다음 토큰을 하나씩 순차적으로 예측하며 문장을 만들어 가는 언어 모델
- 모델의 출력값이 모델의 입력값으로 사용되는 특징 때문에 자기회귀라는 이름이 붙음
- 동작 방식 단계
  1. 시작 토큰 <BOS> 입력
  2. 다음 토큰 확률 분포 예측
  3. 하나를 샘플링 또는 선택
  4. 그 토큰을 다시 입력에 추가
  5. 종료 토큰 <EOS>가 나올 때까지 반복

#### 통계적 언어모델
- 대규모 말뭉치에서 단어(또는 토큰)들이 얼마나 자주, 어떤 순서로 등장하는지를 통계적으로 세어 확률로 표현한 모델
- 핵심은 언어를 규칙이 아니라 빈도와 확률로 본다는 것
- 기존에 학습한 텍스트 데이터에서 패턴을 찾아 확률 분포를 생성
- 이 방법은 단어의 순서와 빈도에만 기초해 문장의 확률을 예측하므로 문맥을 제대로 파악하지 못하면 불완전하거나 부적절한 결과를 생성할 수 있음
- 한 번도 등장한 적이 없는 단어나 문장에 대해서는 정확한 확률을 예측하기가 어려움 (데이터 희소성(Data sparsity)
- 대규모 자연어 데이터를 처리하는 데 효과적이며, 딥러닝 등의 인공지능 기술이 발전하면서 더욱 강력한 모델을 구현할 수 있게 됐음

#### N-gram
- 텍스트에서 N개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정함
