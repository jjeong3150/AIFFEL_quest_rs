# 워드 임베딩
## 벡터화
### Bag of Words(BOW)
- 문장을 “의미의 흐름”이 아니라 “단어의 등장 여부와 빈도”로만 표현하는 가장 기본적인 텍스트 벡터화 방법
- 문장의 순서, 문법, 문맥은 전부 버리고, 어떤 단어가 얼마나 나왔는지만 숫자로 바꿈

<br>

### DTM(문서 단어 행렬, Document-Term Matrix)
- 여러 문서를 행(row), 단어 사전을 열(column) 로 놓고, 각 문서에 각 단어가 얼마나 등장했는지를 숫자로 정리한 행렬
- Bag of Words를 실제로 수학적 형태(행렬)로 만든 것

<br>

### TF-IDF
- TF (Term Frequency): 한 문서 안에서 얼마나 자주 나왔는가
  - <img width="367" height="45" alt="image" src="https://github.com/user-attachments/assets/9fed99e4-419f-4c3c-a61c-38ab213f791b" />
- IDF (Inverse Document Frequency): 전체 문서 중에서 얼마나 희귀한가 → “희귀할수록 중요하다”
  - <img width="213" height="75" alt="image" src="https://github.com/user-attachments/assets/a65c7fd6-1d4d-423a-bce4-9874c0d024c6" />
- 성능이 좋아지는 이유?
  - 불용어(the, is, and 등)가 자동으로 약화됨
  - 문서 특징 단어가 강조됨
  - 분류, 군집, 검색 정확도가 급상승함
- 한계
  - 여전히 단어 순서 무시
  - 의미 관계 무시
  - 문맥 없음
  - 차원 수는 그대로 큼
- 그래서 TF-IDF는 “고전적 머신러닝 + 텍스트” 조합에서 가장 강력

<br>

### 원-핫 인코딩(one-hot encoding)
- 전체 클래스 개수만큼 길이를 가진 벡터를 만들고 해당하는 위치만 1, 나머지는 전부 0으로 둔다
- 장점
  - 구현이 매우 간단
  - 모든 클래스를 동등하게 취급
  - 수학적으로 명확
  - 분류 문제에 직관적
- 단점
  - 차원 폭발
  - 의미 정보 없음
  - 희소 벡터 → 대부분이 0. 메모리 비효율↑
- 모든 임베딩 기법의 조상 같은 개념






